{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d008c6",
   "metadata": {},
   "source": [
    "# Compile and Train a Vision Transformer Model on the Caltech 256 Dataset using a Single Node "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a3be53",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [SageMaker environment](#SageMaker-environment)\n",
    "3. [Working with the Caltech-256 dataset](#Working-with-the-Caltech-256-dataset)   \n",
    "4. [SageMaker Training Job](#SageMaker-Training-Job)  \n",
    "    1. [Training Setup](#Training-Setup)  \n",
    "    2. [Training with Native TensorFlow](#Training-with-Native-TensorFlow)  \n",
    "    3. [Training with Optimized TensorFlow](#Training-with-Optimized-TensorFlow)  \n",
    "5. [Analysis](#Analysis)\n",
    "    1. [Savings from Training Compiler](#Savings-from-Training-Compiler)\n",
    "    2. [Convergence of Training](#Convergence-of-Training)\n",
    "6. [Clean up](#Clean-up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ad24e",
   "metadata": {},
   "source": [
    "## SageMaker Training Compiler Overview\n",
    "\n",
    "SageMaker Training Compiler is a capability of SageMaker that makes hard-to-implement optimizations to reduce training time on GPU instances. The compiler optimizes DL models to accelerate training by more efficiently using SageMaker machine learning (ML) GPU instances. SageMaker Training Compiler is available at no additional charge within SageMaker and can help reduce total billable time as it accelerates training. \n",
    "\n",
    "SageMaker Training Compiler is integrated into the AWS Deep Learning Containers (DLCs). Using the SageMaker Training Compiler enabled AWS DLCs, you can compile and optimize training jobs on GPU instances with minimal changes to your code. Bring your deep learning models to SageMaker and enable SageMaker Training Compiler to accelerate the speed of your training job on SageMaker ML instances for accelerated computing. \n",
    "\n",
    "For more information, see [SageMaker Training Compiler](https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html) in the *Amazon SageMaker Developer Guide*.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this demo, you'll use Amazon SageMaker Training Compiler to train the `Vision Transformer` model on the `Caltech-256` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "**NOTE:** You can run this demo in SageMaker Studio, SageMaker notebook instances, or your local machine with AWS CLI set up. If using SageMaker Studio or SageMaker notebook instances, make sure you choose one of the TensorFlow-based kernels, `Python 3 (TensorFlow x.y Python 3.x CPU Optimized)` or `conda_tesorflow_p39` respectively.\n",
    "\n",
    "**NOTE:** This notebook uses a `ml.p3.2xlarge` instance with a single GPU. However, it can easily be extended to multiple GPUs on a single node. If you don't have enough quota, see [Request a service quota increase for SageMaker resources](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.html#service-limit-increase-request-procedure). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b8d5b",
   "metadata": {},
   "source": [
    "## Development Environment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6666d6",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "This example notebook requires **SageMaker Python SDK v2.92.0**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1a4aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker>=2.92 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (2.94.0)\n",
      "Requirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (1.27.7)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (1.24.7)\n",
      "Requirement already satisfied: awscli in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (1.25.7)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from sagemaker>=2.92) (21.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from sagemaker>=2.92) (1.3.4)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from sagemaker>=2.92) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from sagemaker>=2.92) (1.7.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from sagemaker>=2.92) (0.1.5)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from sagemaker>=2.92) (1.0.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from sagemaker>=2.92) (1.20.3)\n",
      "Requirement already satisfied: attrs==20.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from sagemaker>=2.92) (20.3.0)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from sagemaker>=2.92) (0.2.8)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from sagemaker>=2.92) (3.19.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from botocore) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from botocore) (1.26.8)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from awscli) (0.4.3)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from awscli) (0.15.2)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: PyYAML<5.5,>=3.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from awscli) (5.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from matplotlib) (3.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.92) (3.6.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>=2.92) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from pandas->sagemaker>=2.92) (2021.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from pathos->sagemaker>=2.92) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from pathos->sagemaker>=2.92) (0.3.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from pathos->sagemaker>=2.92) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from pathos->sagemaker>=2.92) (1.6.6.4)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.92\" botocore boto3 awscli matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a93e2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "botocore: 1.27.7\n",
      "boto3: 1.24.7\n",
      "sagemaker: 2.94.0\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "print(f\"botocore: {botocore.__version__}\")\n",
    "print(f\"boto3: {boto3.__version__}\")\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30e484",
   "metadata": {},
   "source": [
    "### SageMaker environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946fe532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::875423407011:role/SageMakerRole\n",
      "sagemaker bucket: sagemaker-us-west-2-875423407011\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# SageMaker session bucket -> used for uploading data, models and logs\n",
    "# SageMaker will automatically create this bucket if it does not exist\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session_bucket}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774ae6f",
   "metadata": {},
   "source": [
    "## Working with the Caltech-256 dataset\n",
    "\n",
    "We have hosted the [Caltech-256](https://authors.library.caltech.edu/7694/) dataset in S3 in us-west-2. We will transfer this dataset to your account and region for use with SageMaker Training.\n",
    "\n",
    "The dataset consists of JPEG images organized into directories with each directory representing an object cateogory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24aea98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "source = 's3://sagemaker-sample-files/datasets/image/caltech-256/256_ObjectCategories'\n",
    "destn = f's3://{sagemaker_session_bucket}/caltech-256'\n",
    "\n",
    "os.system(f'aws s3 sync {source} {destn}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4decc2",
   "metadata": {},
   "source": [
    "## SageMaker Training Job\n",
    "\n",
    "To create a SageMaker training job, we use a `TensorFlow` estimator. Using the estimator, you can define which training script should SageMaker use through `entry_point`, which `instance_type` to use for training, which `hyperparameters` to pass, and so on.\n",
    "\n",
    "When a SageMaker training job starts, SageMaker takes care of starting and managing all the required machine learning instances, picks up the `TensorFlow` Deep Learning Container, uploads your training script, and downloads the data from `sagemaker_session_bucket` into the container at `/opt/ml/input/data`.\n",
    "\n",
    "In the following section, you learn how to set up two versions of the SageMaker `TensorFlow` estimator, a native one without the compiler and an optimized one with the compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f1bba",
   "metadata": {},
   "source": [
    "### Training Setup\n",
    "\n",
    "Set up the basic configuration for training. Set `EPOCHS` to the number of times you would like to loop over the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ed60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRCOMP_IMAGE_URI='763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.9.1-gpu-py39-cu112-ubuntu20.04-sagemaker'\n",
    "EPOCHS = 10\n",
    "INSTANCE_COUNT = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c2c67",
   "metadata": {},
   "source": [
    "### Training with Native TensorFlow\n",
    "\n",
    "The `BATCH_SIZE` in the following code cell is the maximum batch that can fit into the memory of an `ml.p3.2xlarge` instance while giving the best training speed. If you change the model, instance type, and other parameters, you need to do some experiments to find the largest batch size that will fit into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4db276b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'native-tf29-vit-2022-06-13-22-04-26-782'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "BATCH_SIZE = 64 * INSTANCE_COUNT\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "kwargs = dict(\n",
    "    source_dir='scripts',\n",
    "    entry_point='vit_b16_mwms.py',\n",
    "    model_dir=False,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=INSTANCE_COUNT,\n",
    "    image_uri=TRCOMP_IMAGE_URI,\n",
    "    debugger_hook_config=None,\n",
    "    disable_profiler=True,\n",
    "    max_run=60*60, #60 minutes\n",
    "    role = role,\n",
    "    metric_definitions = [\n",
    "        {'Name':'training_loss', 'Regex':'loss: ([0-9.]*?) '},\n",
    "        {'Name':'training_accuracy', 'Regex':'accuracy: ([0-9.]*?) '},\n",
    "        {'Name':'training_latency_per_epoch', 'Regex':'- ([0-9.]*?)s/epoch'},\n",
    "        {'Name':'training_avg_latency_per_step', 'Regex':'- ([0-9.]*?)ms/step'},\n",
    "        {'Name':'training_avg_latency_per_step', 'Regex':'- ([0-9.]*?)s/step'},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure the training job\n",
    "native_estimator = TensorFlow(\n",
    "                    hyperparameters={\n",
    "                        'EPOCHS': EPOCHS,\n",
    "                        'BATCH_SIZE' : BATCH_SIZE,\n",
    "                        'LEARNING_RATE' : LEARNING_RATE,\n",
    "                        'WEIGHT_DECAY' : WEIGHT_DECAY,\n",
    "                        'sagemaker_multi_worker_mirrored_strategy_enabled': True\n",
    "                    },\n",
    "                    base_job_name='native-tf29-vit',\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "# Start training with our uploaded datasets as input\n",
    "native_estimator.fit(inputs=destn, wait=False)\n",
    "\n",
    "# The name of the training job.\n",
    "native_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49b0ac",
   "metadata": {},
   "source": [
    "### Training with Optimized TensorFlow\n",
    "\n",
    "Compilation through Training Compiler changes the memory footprint of the model. Most commonly, this manifests as a reduction in memory utilization and a consequent increase in the largest batch size that can fit on the GPU. But in some case the compiler intelligently promotes caching which leads to a decrease in largest batch size that can fit on the GPU. Note that if you want to change the batch size, you must adjust the learning rate appropriately.\n",
    "\n",
    "**Note:** We recommend you to turn the SageMaker Debugger's profiling and debugging tools off when you use compilation to avoid additional overheads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90d7a995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'optimized-tf29-vit-2022-06-13-22-51-42-662'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Change how TrainingCompilerConfig is used after SDK release\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.training_compiler.config import TrainingCompilerConfig\n",
    "\n",
    "OPTIMIZED_BATCH_SIZE = 48 * INSTANCE_COUNT\n",
    "LEARNING_RATE = LEARNING_RATE / BATCH_SIZE * OPTIMIZED_BATCH_SIZE\n",
    "WEIGHT_DECAY = WEIGHT_DECAY * BATCH_SIZE / OPTIMIZED_BATCH_SIZE\n",
    "\n",
    "# Configure the training job\n",
    "optimized_estimator = TensorFlow(\n",
    "                        hyperparameters={\n",
    "                            TrainingCompilerConfig.HP_ENABLE_COMPILER : True,\n",
    "                            'sagemaker_multi_worker_mirrored_strategy_enabled': True,\n",
    "                            'EPOCHS': EPOCHS,\n",
    "                            'BATCH_SIZE' : OPTIMIZED_BATCH_SIZE,\n",
    "                            'LEARNING_RATE' : LEARNING_RATE,\n",
    "                            'WEIGHT_DECAY' : WEIGHT_DECAY,\n",
    "                        },\n",
    "                        base_job_name='optimized-tf29-vit',\n",
    "                        environment = {\n",
    "                            'NCCL_DEBUG': 'WARN',\n",
    "                        },\n",
    "                        **kwargs,\n",
    "                    )\n",
    "\n",
    "# Start training with our uploaded datasets as input\n",
    "optimized_estimator.fit(inputs=destn, wait=False)\n",
    "\n",
    "# The name of the training job.\n",
    "optimized_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382f1dc",
   "metadata": {},
   "source": [
    "### Wait for training jobs to complete\n",
    "\n",
    "The training jobs described above typically take around 20 mins to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406462ea",
   "metadata": {},
   "source": [
    "**Note:** If the estimator object is no longer available due to a kernel break or refresh, you need to directly use the training job name and manually attach the training job to a new TensorFlow estimator. For example:\n",
    "\n",
    "```python\n",
    "native_estimator = TensorFlow.attach(\"<your_training_job_name>\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50b80394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-06-13 22:11:01 Starting - Preparing the instances for training\n",
      "2022-06-13 22:11:01 Downloading - Downloading input data\n",
      "2022-06-13 22:11:01 Training - Training image download completed. Training in progress..............."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29744/1349542554.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnative_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorFlow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnative_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimized_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorFlow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimized_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mattach\u001b[0;34m(cls, training_job_name, sagemaker_session, model_channel_name)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0mtraining\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m         \"\"\"\n\u001b[0;32m-> 2899\u001b[0;31m         estimator = super(Framework, cls).attach(\n\u001b[0m\u001b[1;32m   2900\u001b[0m             \u001b[0mtraining_job_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_channel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mattach\u001b[0;34m(cls, training_job_name, sagemaker_session, model_channel_name)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         )\n\u001b[1;32m   1130\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1996\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   3196\u001b[0m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedStatusException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m         \"\"\"\n\u001b[0;32m-> 3198\u001b[0;31m         desc = _wait_until_training_done(\n\u001b[0m\u001b[1;32m   3199\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mlast_desc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_train_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3200\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_wait_until_training_done\u001b[0;34m(callable_fn, desc, poll)\u001b[0m\n\u001b[1;32m   4780\u001b[0m     \u001b[0mjob_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4781\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4782\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4783\u001b[0m         \u001b[0mjob_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4784\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjob_desc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "native_estimator = TensorFlow.attach(native_estimator.latest_training_job.name)\n",
    "optimized_estimator = TensorFlow.attach(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca086c25",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Here we view the training metrics from the training jobs as a Pandas dataframe\n",
    "\n",
    "#### Native TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d431855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: No metrics called training_avg_latency_per_step found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_loss</th>\n",
       "      <th>training_accuracy</th>\n",
       "      <th>training_latency_per_epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.6932</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.6304</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6181</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.6120</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.6138</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.6187</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.6018</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.6205</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.6059</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.6079</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>290.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        training_loss  training_accuracy  training_latency_per_epoch\n",
       "epochs                                                              \n",
       "1              5.6932             0.0236                       335.0\n",
       "2              5.6304             0.0261                       289.0\n",
       "3              5.6181             0.0272                       288.0\n",
       "4              5.6120             0.0246                       289.0\n",
       "5              5.6138             0.0273                       289.0\n",
       "6              5.6187             0.0262                       289.0\n",
       "7              5.6018             0.0262                       289.0\n",
       "8              5.6205             0.0263                       289.0\n",
       "9              5.6059             0.0277                       294.0\n",
       "10             5.6079             0.0263                       290.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract training metrics from the estimator\n",
    "native_metrics = native_estimator.training_job_analytics.dataframe()\n",
    "\n",
    "# Restructure table for viewing\n",
    "for metric in native_metrics['metric_name'].unique():\n",
    "    native_metrics[metric] = native_metrics[native_metrics['metric_name']==metric]['value']\n",
    "native_metrics = native_metrics.drop(columns=['metric_name', 'value'])\n",
    "native_metrics = native_metrics.groupby('timestamp').max()\n",
    "native_metrics['epochs'] = range(1,EPOCHS+1)\n",
    "native_metrics = native_metrics.set_index('epochs')\n",
    "\n",
    "native_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b38ddb",
   "metadata": {},
   "source": [
    "#### Optimized TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract training metrics from the estimator\n",
    "optimized_metrics = optimized_estimator.training_job_analytics.dataframe()\n",
    "\n",
    "# Restructure table for viewing\n",
    "for metric in optimized_metrics['metric_name'].unique():\n",
    "    optimized_metrics[ metric] = optimized_metrics[optimized_metrics['metric_name']==metric]['value']\n",
    "optimized_metrics = optimized_metrics.drop(columns=['metric_name', 'value'])\n",
    "optimized_metrics = optimized_metrics.groupby('timestamp').max()\n",
    "optimized_metrics['epochs'] = range(1,11)\n",
    "optimized_metrics = optimized_metrics.set_index('epochs')\n",
    "\n",
    "optimized_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999562c",
   "metadata": {},
   "source": [
    "### Savings from Training Compiler\n",
    "\n",
    "Let us calculate the actual savings on the training jobs above and the potential for savings for a longer training job.\n",
    "\n",
    "#### Actual Savings\n",
    "\n",
    "To get the actual savings, we use the describe_training_job API to get the billable seconds for each training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69e0d877",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'BillableTimeInSeconds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29744/777556099.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnative_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnative_secs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BillableTimeInSeconds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnative_secs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'BillableTimeInSeconds'"
     ]
    }
   ],
   "source": [
    "# Billable seconds for the Native TensorFlow Training job\n",
    "\n",
    "details = sess.describe_training_job(job_name=native_estimator.latest_training_job.name)\n",
    "native_secs = details['BillableTimeInSeconds']\n",
    "\n",
    "native_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Billable seconds for the Optimized TensorFlow Training job\n",
    "\n",
    "details = sess.describe_training_job(job_name=optimized_estimator.latest_training_job.name)\n",
    "optimized_secs = details['BillableTimeInSeconds']\n",
    "\n",
    "optimized_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating percentage Savings from Training Compiler\n",
    "\n",
    "percentage = (native_secs-optimized_secs)*100/native_secs\n",
    "\n",
    "f\"Training Compiler yielded {percentage:.2f}% savings in training cost.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5661dfe",
   "metadata": {},
   "source": [
    "#### Potential savings\n",
    "\n",
    "The Training Compiler works by compiling the model graph once per input shape and reusing the cached graph for subsequent steps. As a result the first few steps of training incur an increased latency owing to compilation which we refer to as the compilation overhead. This overhead is amortized over time thanks to the subsequent steps being much faster. We will demonstrate this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7e90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(native_metrics['training_latency_per_epoch'], label='native_epoch_latency')\n",
    "plt.plot(optimized_metrics['training_latency_per_epoch'], label='optimized_epoch_latency')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e9288",
   "metadata": {},
   "source": [
    "We calculate the potential savings below from the difference in steady state epoch latency between native TensorFlow and optimized TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a33c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "native_steady_state_latency = native_metrics['training_latency_per_epoch'].iloc[-1]\n",
    "\n",
    "native_steady_state_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7185dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_steady_state_latency = optimized_metrics['training_latency_per_epoch'].iloc[-1]\n",
    "\n",
    "optimized_steady_state_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating potential percentage Savings from Training Compiler\n",
    "\n",
    "percentage = (native_steady_state_latency-optimized_steady_state_latency)*100/native_steady_state_latency\n",
    "\n",
    "f\"Training Compiler can potentially yield {percentage:.2f}% savings in training cost for a longer training job.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b06fc2",
   "metadata": {},
   "source": [
    "### Convergence of Training\n",
    "\n",
    "Training Compiler brings down total training time by intelligently choosing between memory utilization and core utilization in the GPU. This does not have any effect on the model arithmetic and consequently convergence of the model.\n",
    "\n",
    "However, since we are working with a new batch size, hyperparameters like - learning rate, learning rate schedule and weight decay might have to be scaled and tuned for the new batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfeb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(native_metrics['training_loss'], label='native_loss')\n",
    "plt.plot(optimized_metrics['training_loss'], label='optimized_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd8835",
   "metadata": {},
   "source": [
    "We can see that the model's convergence behavior is similar with and without Training Compiler. Here we have tuned the batch size specific hyperparameters - Learning Rate and Weight Decay using a linear scaling.\n",
    "\n",
    "Learning rate is directly proportional to the batch size:\n",
    "```python\n",
    "new_learning_rate = old_learning_rate * new_batch_size/old_batch_size\n",
    "```\n",
    "\n",
    "Weight decay is inversely proportional to the batch size:\n",
    "```python\n",
    "new_weight_decay = old_weight_decay * old_batch_size/new_batch_size\n",
    "```\n",
    "\n",
    "Better results can be achieved with further tuning. Check out [Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) for tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc219a22",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Stop all training jobs launched if the jobs are still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f55df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_training_job(name):\n",
    "    status = sess.describe_training_job(name)[\"TrainingJobStatus\"]\n",
    "    if status == \"InProgress\":\n",
    "        sm.stop_training_job(TrainingJobName=name)\n",
    "\n",
    "\n",
    "stop_training_job(native_estimator.latest_training_job.name)\n",
    "stop_training_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e6c99",
   "metadata": {},
   "source": [
    "Also, to find instructions on cleaning up resources, see [Clean Up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html) in the *Amazon SageMaker Developer Guide*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

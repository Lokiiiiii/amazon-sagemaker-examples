{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536d9f47",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Permissions](#Permissions)\n",
    "3. [SageMaker Training Job](#SageMaker-Training-Job)  \n",
    "    1. [Training with Native TensorFlow](#NativeTF)  \n",
    "    2. [Training with Optimized TensorFlow](#OptimizedTF)  \n",
    "    3. [Analysis](#Analysis)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7014fcc4",
   "metadata": {},
   "source": [
    "# Compile and Train the GPT2 Model using the Transformers Trainer API with the SST2 Dataset for Single-Node Multi-GPU Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6772e3d",
   "metadata": {},
   "source": [
    "## SageMaker Training Compiler Overview\n",
    "\n",
    "SageMaker Training Compiler is a capability of SageMaker that makes these hard-to-implement optimizations to reduce training time on GPU instances. The compiler optimizes Deep Learning (DL) models to accelerate training by more efficiently using SageMaker machine learning (ML) GPU instances. SageMaker Training Compiler is available at no additional charge within SageMaker and can help reduce total billable time as it accelerates training. \n",
    "\n",
    "SageMaker Training Compiler is integrated into the AWS Deep Learning Containers (DLCs). Using the SageMaker Training Compiler enabled AWS DLCs, you can compile and optimize training jobs on GPU instances with minimal changes to your code. Bring your deep learning models to SageMaker and enable SageMaker Training Compiler to accelerate the speed of your training job on SageMaker ML instances for accelerated computing. \n",
    "\n",
    "For more information, see [SageMaker Training Compiler](https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html) in the *Amazon SageMaker Developer Guide*.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this demo, you'll use Hugging Face's `transformers` and `datasets` libraries with Amazon SageMaker Training Compiler to train the `gpt-2` model on the `Stanford Sentiment Treebank v2 (SST2)` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "**NOTE:** You can run this demo in SageMaker Studio, SageMaker notebook instances, or your local machine with AWS CLI set up. If using SageMaker Studio or SageMaker notebook instances, make sure you choose one of the TensorFlow-based kernels, `Python 3 (TensorFlow x.y Python 3.x CPU Optimized)` or `conda_tensorflow_p36` respectively.\n",
    "\n",
    "**NOTE:** This notebook uses two `ml.p3.8xlarge` instances that have multiple GPUs. If you don't have enough quota, see [Request a service quota increase for SageMaker resources](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.html#service-limit-increase-request-procedure). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21bcb9",
   "metadata": {},
   "source": [
    "# Development Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d435a",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "This example notebook requires the **SageMaker Python SDK v2.70.0** and **transformers v4.11.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c31f9214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting sagemaker==2.70.0\n",
      "  Using cached sagemaker-2.70.0-py2.py3-none-any.whl\n",
      "Collecting smdebug-rulesconfig==1.0.1\n",
      "  Using cached smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pathos\n",
      "  Using cached pathos-0.2.8-py2.py3-none-any.whl (81 kB)\n",
      "Collecting importlib-metadata>=1.4.0\n",
      "  Downloading importlib_metadata-4.10.0-py3-none-any.whl (17 kB)\n",
      "Collecting google-pasta\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting boto3>=1.20.18\n",
      "  Downloading boto3-1.20.25-py3-none-any.whl (131 kB)\n",
      "     |████████████████████████████████| 131 kB 8.8 MB/s            \n",
      "\u001b[?25hCollecting protobuf3-to-dict>=0.1.5\n",
      "  Using cached protobuf3_to_dict-0.1.5-py3-none-any.whl\n",
      "Collecting attrs\n",
      "  Using cached attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "     |████████████████████████████████| 11.3 MB 103.3 MB/s            \n",
      "\u001b[?25hCollecting protobuf>=3.1\n",
      "  Using cached protobuf-3.19.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting numpy>=1.9.0\n",
      "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "     |████████████████████████████████| 15.7 MB 65.9 MB/s            \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.24.0,>=1.23.25\n",
      "  Downloading botocore-1.23.25-py3-none-any.whl (8.5 MB)\n",
      "     |████████████████████████████████| 8.5 MB 43.2 MB/s            \n",
      "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
      "  Using cached s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting typing-extensions>=3.6.4\n",
      "  Using cached typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Using cached pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting ppft>=1.6.6.4\n",
      "  Using cached ppft-1.6.6.4-py3-none-any.whl (65 kB)\n",
      "Collecting multiprocess>=0.70.12\n",
      "  Using cached multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
      "Collecting pox>=0.3.0\n",
      "  Using cached pox-0.3.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting dill>=0.3.4\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Installing collected packages: six, urllib3, python-dateutil, jmespath, dill, botocore, zipp, typing-extensions, s3transfer, pytz, pyparsing, protobuf, ppft, pox, numpy, multiprocess, smdebug-rulesconfig, protobuf3-to-dict, pathos, pandas, packaging, importlib-metadata, google-pasta, boto3, attrs, sagemaker\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.7\n",
      "    Uninstalling urllib3-1.26.7:\n",
      "      Successfully uninstalled urllib3-1.26.7\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 0.10.0\n",
      "    Uninstalling jmespath-0.10.0:\n",
      "      Successfully uninstalled jmespath-0.10.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.23.21\n",
      "    Uninstalling botocore-1.23.21:\n",
      "      Successfully uninstalled botocore-1.23.21\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.6.0\n",
      "    Uninstalling zipp-3.6.0:\n",
      "      Successfully uninstalled zipp-3.6.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.0.1\n",
      "    Uninstalling typing-extensions-4.0.1:\n",
      "      Successfully uninstalled typing-extensions-4.0.1\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.5.0\n",
      "    Uninstalling s3transfer-0.5.0:\n",
      "      Successfully uninstalled s3transfer-0.5.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2021.3\n",
      "    Uninstalling pytz-2021.3:\n",
      "      Successfully uninstalled pytz-2021.3\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.6\n",
      "    Uninstalling pyparsing-3.0.6:\n",
      "      Successfully uninstalled pyparsing-3.0.6\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "  Attempting uninstall: ppft\n",
      "    Found existing installation: ppft 1.6.6.4\n",
      "    Uninstalling ppft-1.6.6.4:\n",
      "      Successfully uninstalled ppft-1.6.6.4\n",
      "  Attempting uninstall: pox\n",
      "    Found existing installation: pox 0.3.0\n",
      "    Uninstalling pox-0.3.0:\n",
      "      Successfully uninstalled pox-0.3.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.4\n",
      "    Uninstalling numpy-1.21.4:\n",
      "      Successfully uninstalled numpy-1.21.4\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.12.2\n",
      "    Uninstalling multiprocess-0.70.12.2:\n",
      "      Successfully uninstalled multiprocess-0.70.12.2\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: protobuf3-to-dict\n",
      "    Found existing installation: protobuf3-to-dict 0.1.5\n",
      "    Uninstalling protobuf3-to-dict-0.1.5:\n",
      "      Successfully uninstalled protobuf3-to-dict-0.1.5\n",
      "  Attempting uninstall: pathos\n",
      "    Found existing installation: pathos 0.2.8\n",
      "    Uninstalling pathos-0.2.8:\n",
      "      Successfully uninstalled pathos-0.2.8\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.4\n",
      "    Uninstalling pandas-1.3.4:\n",
      "      Successfully uninstalled pandas-1.3.4\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.8.2\n",
      "    Uninstalling importlib-metadata-4.8.2:\n",
      "      Successfully uninstalled importlib-metadata-4.8.2\n",
      "  Attempting uninstall: google-pasta\n",
      "    Found existing installation: google-pasta 0.2.0\n",
      "    Uninstalling google-pasta-0.2.0:\n",
      "      Successfully uninstalled google-pasta-0.2.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.20.21\n",
      "    Uninstalling boto3-1.20.21:\n",
      "      Successfully uninstalled boto3-1.20.21\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.2.0\n",
      "    Uninstalling attrs-21.2.0:\n",
      "      Successfully uninstalled attrs-21.2.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.70.0\n",
      "    Uninstalling sagemaker-2.70.0:\n",
      "      Successfully uninstalled sagemaker-2.70.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-serving-api 1.15.0 requires tensorflow~=1.15.0, which is not installed.\n",
      "anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.\n",
      "awscli 1.22.21 requires botocore==1.23.21, but you have botocore 1.23.25 which is incompatible.\n",
      "aiobotocore 1.3.0 requires botocore<1.20.50,>=1.20.49, but you have botocore 1.23.25 which is incompatible.\n",
      "tensorflow-cpu 1.15.5 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\u001b[0m\n",
      "Successfully installed attrs-21.2.0 boto3-1.20.25 botocore-1.23.25 dill-0.3.4 google-pasta-0.2.0 importlib-metadata-4.10.0 jmespath-0.10.0 multiprocess-0.70.12.2 numpy-1.21.5 packaging-21.3 pandas-1.3.5 pathos-0.2.8 pox-0.3.0 ppft-1.6.6.4 protobuf-3.19.1 protobuf3-to-dict-0.1.5 pyparsing-3.0.6 python-dateutil-2.8.2 pytz-2021.3 s3transfer-0.5.0 sagemaker-2.70.0 six-1.16.0 smdebug-rulesconfig-1.0.1 typing-extensions-4.0.1 urllib3-1.26.7 zipp-3.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (1.23.25)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (1.20.25)\n",
      "Requirement already satisfied: awscli in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (1.22.21)\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.22.25-py3-none-any.whl (3.8 MB)\n",
      "     |████████████████████████████████| 3.8 MB 5.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from botocore) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from botocore) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from boto3) (0.5.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from awscli) (0.15.2)\n",
      "Requirement already satisfied: colorama<0.4.4,>=0.2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from awscli) (0.4.3)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: PyYAML<5.5,>=3.10 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from awscli) (5.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Installing collected packages: awscli\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.22.21\n",
      "    Uninstalling awscli-1.22.21:\n",
      "      Successfully uninstalled awscli-1.22.21\n",
      "Successfully installed awscli-1.22.25\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall sagemaker==2.70.0\n",
    "!pip install botocore boto3 awscli --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "77126c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers==4.11.0 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (4.11.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (4.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (0.0.46)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (1.21.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (2020.11.13)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (21.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from transformers==4.11.0) (0.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.11.0) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from importlib-metadata->transformers==4.11.0) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from requests->transformers==4.11.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from requests->transformers==4.11.0) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from requests->transformers==4.11.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from requests->transformers==4.11.0) (1.26.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.11.0) (1.16.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.11.0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.11.0) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ae2c18c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker: 2.70.0\n",
      "transformers: 4.11.0\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57011894",
   "metadata": {},
   "source": [
    "### SageMaker environment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e34612a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::875423407011:role/SageMakerRole\n",
      "sagemaker bucket: sagemaker-us-west-2-875423407011\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# SageMaker session bucket -> used for uploading data, models and logs\n",
    "# SageMaker will automatically create this bucket if it does not exist\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e7e3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd0ddc",
   "metadata": {},
   "source": [
    "## SageMaker Training Job\n",
    "\n",
    "To create a SageMaker training job, we use a `HuggingFace` estimator. Using the estimator, you can define which fine-tuning script should SageMaker use through `entry_point`, which `instance_type` to use for training, which `hyperparameters` to pass, and so on.\n",
    "\n",
    "When a SageMaker training job starts, SageMaker takes care of starting and managing all the required machine learning instances, picks up the `HuggingFace` Deep Learning Container, uploads your training script, and downloads the data from `sagemaker_session_bucket` into the container at `/opt/ml/input/data`.\n",
    "\n",
    "In the following section, you learn how to set up two versions of the SageMaker `HuggingFace` estimator, a native one without the compiler and an optimized one with the compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b0a74",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e3079eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we configure the training job. Please configure the appropriate options below:\n",
    "\n",
    "EPOCHS = 140\n",
    "\n",
    "\n",
    "# Choose between Causal Language Model and Masked Language Model\n",
    "LANGUAGE_MODELING_LOSS = \"clm\"  # or \"clm\"\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "TOKENIZER_NAME = \"gpt2\"\n",
    "MODEL_CONFIG = \"model_type\"\n",
    "SEQ_LEN = 128\n",
    "\n",
    "# For more information about the options, please look into the training scripts\n",
    "\n",
    "# SageMaker Training Compiler currently only supports training on GPU\n",
    "# Select Instance type for training\n",
    "INSTANCE_TYPE = \"ml.p3.8xlarge\"  # ml.p3.8xlarge is easily available. However, p3.16xlarge provides better performance.\n",
    "NUM_GPUS_PER_INSTANCE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6d20e",
   "metadata": {},
   "source": [
    "<a id='NativeTF'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae4cf8",
   "metadata": {},
   "source": [
    "### Training with Native TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217330c",
   "metadata": {},
   "source": [
    "The batch size below is the maximum batch we could fit into the memory of an Nvidia V100 GPU (P3). If you change the model, instance type or sequence length etc., please experiment to find the largest batch size that will fit into memory. \n",
    "\n",
    "This example uses a modified version of HuggingFace training script `run_clm.py`, which you can find inside the `scripts` folder. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "native_batch_size = 48\n",
    "learning_rate_native = float(\"1.25e-5\")\n",
    "scaled_learning_rate = learning_rate_native * NUM_GPUS_PER_INSTANCE\n",
    "\n",
    "# hyperparameters are passed to the training entrypoint as arguments\n",
    "hyperparameters = {\n",
    "    \"model_name_or_path\": MODEL_NAME,\n",
    "    \"output_dir\": \"/opt/ml/model\",\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config_name\": \"sst2\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"per_device_train_batch_size\": native_batch_size,\n",
    "    \"learning_rate\": scaled_learning_rate,\n",
    "    \"max_seq_length\": SEQ_LEN,\n",
    "    \"num_train_epochs\": EPOCHS,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "}\n",
    "\n",
    "# configure the training job\n",
    "native_estimator = HuggingFace(\n",
    "    entry_point=f\"run_{LANGUAGE_MODELING_LOSS}.py\",\n",
    "    source_dir=\"./scripts\",\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    base_job_name=f\"{MODEL_NAME.upper()}-{native_batch_size}batch-{EPOCHS}epoch-{SEQ_LEN}seq\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    volume_size=100,\n",
    "    py_version=\"py37\",\n",
    "    transformers_version=\"4.11.0\",\n",
    "    tensorflow_version=\"2.5.1\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_profiler=True,  # Disabling SageMaker Profiler to avoid overheads during benchmarking\n",
    "    debugger_hook_config=False,  # Disabling SageMaker Debugger to avoid overheads during benchmarking\n",
    ")\n",
    "\n",
    "# start the training job\n",
    "native_estimator.fit(wait=False)\n",
    "native_estimator.latest_training_job.name\n",
    "native_waiter = native_estimator.sagemaker_session.sagemaker_client.get_waiter(\n",
    "    \"training_job_completed_or_stopped\"\n",
    ")\n",
    "native_waiter.wait(TrainingJobName=native_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52857120",
   "metadata": {},
   "source": [
    "<a id='OptimizedTF'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b01f74",
   "metadata": {},
   "source": [
    "### Training with Optimized TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6e7bd",
   "metadata": {},
   "source": [
    "Compilation through Training Compiler changes the memory footprint of the model. Note that if you want to change the batch size, you must adjust the learning rate appropriately.\n",
    "\n",
    "**Note:** We recommend you to turn the SageMaker Debugger's profiling and debugging tools off when you use compilation to avoid additional overheads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4138ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# SageMaker Training Compiler has increased the memory consumption of the model leading to decrease in batch size.\n",
    "hyperparameters[\"per_device_train_batch_size\"] = 32\n",
    "\n",
    "# The original LR was set for a batch of 48. Here we are scaling learning rate with batch size.\n",
    "hyperparameters[\"learning_rate\"] = (\n",
    "    learning_rate_native\n",
    "    * NUM_GPUS_PER_INSTANCE\n",
    "    / 48\n",
    "    * hyperparameters[\"per_device_train_batch_size\"]\n",
    ")\n",
    "\n",
    "# configure the training job\n",
    "optimized_estimator = HuggingFace(\n",
    "    entry_point=f\"run_{LANGUAGE_MODELING_LOSS}.py\",\n",
    "    compiler_config=TrainingCompilerConfig(),\n",
    "    source_dir=\"./scripts\",\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    base_job_name=f\"{MODEL_NAME.upper()}XLA-{hyperparameters['per_device_train_batch_size']}batch-{EPOCHS}epoch-{SEQ_LEN}seq\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    volume_size=100,\n",
    "    py_version=\"py37\",\n",
    "    transformers_version=\"4.11.0\",\n",
    "    tensorflow_version=\"2.5.1\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_profiler=True,  # Disabling SageMaker Profiler to avoid overheads during benchmarking\n",
    "    debugger_hook_config=False,  # Disabling SageMaker Debugger to avoid overheads during benchmarking\n",
    ")\n",
    "\n",
    "# start the training job\n",
    "optimized_estimator.fit(wait=False)\n",
    "optimized_estimator.latest_training_job.name\n",
    "optimized_waiter = optimized_estimator.sagemaker_session.sagemaker_client.get_waiter(\n",
    "    \"training_job_completed_or_stopped\"\n",
    ")\n",
    "optimized_waiter.wait(TrainingJobName=optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f7c0b",
   "metadata": {},
   "source": [
    "### Wait for training jobs to complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "native_estimator = HuggingFace.attach(native_estimator.latest_training_job.name)\n",
    "optimized_estimator = HuggingFace.attach(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bbb67",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1db8c",
   "metadata": {},
   "source": [
    "**Note:** If the estimator object is no longer available due to a kernel break or refresh, you need to directly use the training job name and manually attach the training job to a new HuggingFace estimator. For example:\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace.attach(\"<your_huggingface_training_job_name>\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47efb6",
   "metadata": {},
   "source": [
    "### Load logs of the training job *with* SageMaker Training Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173fe11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture optimized\n",
    "\n",
    "# access the logs of the optimized training job\n",
    "optimized_estimator.sagemaker_session.logs_for_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859a9e2",
   "metadata": {},
   "source": [
    "### Load logs of the training job *without* SageMaker Training Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fcef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture native\n",
    "\n",
    "# access the logs of the native training job\n",
    "native_estimator.sagemaker_session.logs_for_job(native_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9023bf0",
   "metadata": {},
   "source": [
    "### Create helper functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ba826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def _summarize(captured):\n",
    "    final = []\n",
    "    for line in captured.stdout.replace(\"#010\", \"\").replace(\"]\", \"\\n\").replace(\"-\", \"\").split(\"\\n\"):\n",
    "        cleaned = line.strip()\n",
    "        if (\n",
    "            cleaned.startswith(\"ETA\")\n",
    "            or \"*\" * 5 in cleaned\n",
    "            or \"ms/step\" in cleaned\n",
    "            or \"Epoch\" in cleaned\n",
    "            or (\"INFO\" in cleaned and \"=\" in cleaned)\n",
    "        ):\n",
    "            final.append(cleaned)\n",
    "    return final\n",
    "\n",
    "\n",
    "def make_sense(string):\n",
    "    try:\n",
    "        return literal_eval(string.split(chr(27))[0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def summarize(summary):\n",
    "    final = {\"train\": {}}\n",
    "    phase = \"train\"\n",
    "    for line in summary:\n",
    "        if \"Epoch\" in line:\n",
    "            epoch = literal_eval(line.split(\" \")[1].split(\"/\")[0])\n",
    "            if epoch not in final[phase]:\n",
    "                final[phase][epoch] = {\"loss\": [], \"ms/step\": -1}\n",
    "        elif line.startswith(\"ETA\"):\n",
    "            try:\n",
    "                extract = line[line.index(\"loss:\") : line.index(\"loss:\") + 49]\n",
    "                values = [i for i in extract.split(\" \") if make_sense(i)]\n",
    "                loss = values[0].split(chr(27))[0]\n",
    "                final[phase][epoch][\"loss\"].append(loss)\n",
    "            except:\n",
    "                pass\n",
    "        elif \"ms/step\" in line:\n",
    "            avg_step_latency = make_sense(\n",
    "                [i for i in line.split(\" \") if \"ms/step\" in i][0].replace(\"ms/step\", \"\")\n",
    "            )\n",
    "            final[phase][epoch][\"ms/step\"] = avg_step_latency\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b281d3",
   "metadata": {},
   "source": [
    "### Convergence of Training Loss\n",
    "\n",
    "SageMaker Training Compiler does not affect the model convergence behavior. Here, we see the decrease in training loss is similar with and without SageMaker Training Compiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_across_epochs(data_summary):\n",
    "    loss = []\n",
    "    epochs = []\n",
    "    for epoch, value in data_summary[\"train\"].items():\n",
    "        if value[\"loss\"]:\n",
    "            epochs.append(epoch)\n",
    "            loss.append(literal_eval(value[\"loss\"][0]))\n",
    "    return epochs, loss\n",
    "\n",
    "\n",
    "optimized_epochs, optimized_losses = get_loss_across_epochs(summarize(_summarize(optimized)))\n",
    "native_epochs, native_losses = get_loss_across_epochs(summarize(_summarize(native)))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.title(\"Plot of Training Loss\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(native_epochs, native_losses, label=\"Baseline TF\")\n",
    "\n",
    "plt.plot(optimized_epochs, optimized_losses, label=\"SM Training Compiler Enhanced TF\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c8001a",
   "metadata": {},
   "source": [
    "### Total Billable Time\n",
    "\n",
    "Finally, the decrease in total training time results in a decrease in the billable seconds from SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5094dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BillableTimeInSeconds(name):\n",
    "    describe_training_job = (\n",
    "        optimized_estimator.sagemaker_session.sagemaker_client.describe_training_job\n",
    "    )\n",
    "    details = describe_training_job(TrainingJobName=name)\n",
    "    return details[\"BillableTimeInSeconds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Billable = {}\n",
    "Billable[\"Native\"] = BillableTimeInSeconds(native_estimator.latest_training_job.name)\n",
    "Billable[\"Optimized\"] = BillableTimeInSeconds(optimized_estimator.latest_training_job.name)\n",
    "pd.DataFrame(Billable, index=[\"BillableSecs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ed9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = (Billable[\"Native\"] - Billable[\"Optimized\"]) * 100 / Billable[\"Native\"]\n",
    "print(f\"SageMaker Training Compiler integrated TensorFlow was {int(speedup)}% faster in summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e91e0",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Stop all training jobs launched if the jobs are still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def stop_training_job(name):\n",
    "    status = sm.describe_training_job(TrainingJobName=name)[\"TrainingJobStatus\"]\n",
    "    if status == \"InProgress\":\n",
    "        sm.stop_training_job(TrainingJobName=name)\n",
    "\n",
    "\n",
    "stop_training_job(native_estimator.latest_training_job.name)\n",
    "stop_training_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7230ba4",
   "metadata": {},
   "source": [
    "Also, to find instructions on cleaning up resources, see [Clean Up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html) in the *Amazon SageMaker Developer Guide*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p37",
   "language": "python",
   "name": "conda_tensorflow_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
